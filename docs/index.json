[
{
	"uri": "//localhost:1313/",
	"title": "Session Management",
	"tags": [],
	"description": "",
	"content": "SCALING APPLICATION WITH KUBERNETES Overall Horizontal Pod Autoscaler (HPA) in Kubernetes can use Metric Server to scale pods automatically, only supporting metrics such as CPU, memory. Therefore, if applications want to automatically scale according to their own metrics, Kubernetes will not support it. Problem: I have applications running on tomcat installed on kubernetes. I want these applications to automatically scale to handle the rapid increase in requests. SOLUTION: I use the metric is tomcat_requestcount_total and Prometheus tools, Prometheus-Adapter combined with HPA to scale according to Custom metrics.\nThis is the overall architecture of scale. Content Kubernetes Architecture Setup Lab Monitoring Manage session logs "
},
{
	"uri": "//localhost:1313/1-kubernetesarchitecture/",
	"title": "Kubernetes Architecture",
	"tags": [],
	"description": "",
	"content": "This section is a quick introduction to the architecture and how components communicate in a kubernetes network.\nContent Kubernetes Components Kubernetes Networking CILIUM "
},
{
	"uri": "//localhost:1313/1-kubernetesarchitecture/1.1-kubernetescomponents/",
	"title": "Kubernetes Components",
	"tags": [],
	"description": "",
	"content": "A Kubernetes cluster consists of a set of worker machines, called nodes, that run containerized applications. Every cluster has at least one worker node. The worker node(s) host the Pods that are the components of the application workload. The control plane manages the worker nodes and the Pods in the cluster. In production environments, the control plane usually runs across multiple computers and a cluster usually runs multiple nodes, providing fault-tolerance and high availability.\nIn this lab, I only developed a model that only includes 3 nodes, which does not meet high availability. If possible, I will deploy it in the next lab or you can read it in the article below.\nCreating Highly Available Clusters with kubeadm Next we will go through the components of kubernetes to understand the functions of each component that we will deploy in this lab.\n1.Control Plane Components The control plane is responsible for making decisions related to resource allocation, job scheduling, and network traffic routing. For example, in Kubernetes, the control plane decides where to deploy containers based on available resources. Control plane components can be run on any machine in the cluster. However, for simplicity, setup scripts typically start all control plane components on the same machine, and do not run user containers on this machine. kube-apiserver\nIt provides a RESTful HTTP API that users, management tools, and other Kubernetes components (such as kube-controller-manager, kube-scheduler, and kubelet) use to communicate with the cluster. Any requests related to the cluster\u0026rsquo;s state or configuration go through kube-apiserver. etcd\netcd stores all Kubernetes state data, including information about Pods, Nodes, ConfigMaps, Secrets, and many other resources. Every change in the cluster, such as creating, updating, or deleting resources, is recorded in etcd.\netcd is designed with High Consistency and Availability in mind to ensure data consistency across the entire cluster, even if failures occur. It uses the Raft algorithm to maintain data consistency between replicas in an etcd cluster. This ensures that data is always up to date and available even if one or more nodes fail.\nkube-scheduler\nit is responsible for scheduling Pods so that they can run on Nodes in the cluster.\nWhen a new Pod is created that is not assigned to any Node, the kube-scheduler decides which Node in the cluster is best suited to run that Pod. This decision is based on many factors such as available resources (CPU, RAM), the resource requirements of the Pod, and other constraints.\nkube-controller-manager\nControllers continuously monitor the current state of resources in the cluster and adjust them to match the desired state. For example, if the number of running Pods is lower than the desired number, kube-controller-manager will create more Pods to meet the demand.\nkube-controller-manager is responsible for troubleshooting problems in the cluster. If a Pod, Node, or other resource fails, controllers will respond by restarting that resource or creating a new one.\n2.Node Components Kubelet\nAn agent that runs on each node in the cluster. It makes sure that containers are running in a Pod.\nThe kubelet takes a set of PodSpecs that are provided through various mechanisms and ensures that the containers described in those PodSpecs are running and healthy. The kubelet doesn\u0026rsquo;t manage containers which were not created by Kubernetes.\nKube-proxy\nkube-proxy sets up rules to forward network traffic to Pods. When a Service is created, kube-proxy creates iptables or ipvs rules on each Node to forward traffic to the Pods behind that Service. This distributes traffic evenly across Pods, providing simple load balancing.\nkube-proxy continuously monitors changes in the Kubernetes API to update its network rules accordingly. When there are changes to the Service\u0026rsquo;s configuration, such as adding or removing Pods, kube-proxy adjusts the network rules to ensure that traffic is forwarded correctly.\nContainer runtime\nThe container runtime is responsible for creating and launching containers from images. It provides an isolated environment for containers, ensuring that applications inside the container can run independently and do not interfere with the system or other containers. It also manages the stopping and deletion of containers when they are no longer needed or when requested by Kubernetes. Kubernetes interacts with the container runtime through the Container Runtime Interface (CRI). There are different types of container runtimes and in this lab I\u0026rsquo;m using Containerd. "
},
{
	"uri": "//localhost:1313/3-accessibilitytoinstances/3.1-prometheus/",
	"title": "Prometheus",
	"tags": [],
	"description": "",
	"content": "Prometheus Monitoring Setup on Kubernetes Prometheus Kubernetes Manifest Files I will use these sample configuration files hosted on Github and reconfigure a bit. You can clone the repository using the following command. git clone https://github.com/techiescamp/kubernetes-prometheus Create a Namespace \u0026amp; ClusterRole First, we will create a Kubernetes namespace for all our monitoring components. If you don’t create a dedicated namespace, all the Prometheus kubernetes deployment objects get deployed on the default namespace. Execute the following command to create a new namespace named monitoring. kubectl create namespace monitoring Prometheus uses Kubernetes APIs to read all the available metrics from Nodes, Pods, Deployments, etc. For this reason, we need to create an RBAC policy with read access to required API groups and bind the policy to the monitoring namespace.\nCreate RBAC from default clusterRole.yaml file clusterRole.yaml apiVersion: rbac.authorization.k8s.io/v1\rkind: ClusterRole\rmetadata:\rname: prometheus\rrules:\r- apiGroups: [\u0026#34;\u0026#34;]\rresources:\r- nodes\r- nodes/proxy\r- services\r- endpoints\r- pods\rverbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;]\r- apiGroups:\r- extensions\rresources:\r- ingresses\rverbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;]\r- nonResourceURLs: [\u0026#34;/metrics\u0026#34;]\rverbs: [\u0026#34;get\u0026#34;]\r---\rapiVersion: rbac.authorization.k8s.io/v1\rkind: ClusterRoleBinding\rmetadata:\rname: prometheus\rroleRef:\rapiGroup: rbac.authorization.k8s.io\rkind: ClusterRole\rname: prometheus\rsubjects:\r- kind: ServiceAccount\rname: default\rnamespace: monitoring A little explanation about the above role, the default configurations are more than enough for this lab, you can see that get, list and watch permissions are added to nodes, service endpoints, pods and ingress. The role binding is bound to the monitor namespace. If you have any use case to get metrics from any other object, you need to add that to this cluster role.\nCreate the role using the following command.\nkubectl create -f clusterRole.yaml Create a Config Map To Externalize Prometheus Configurations All configurations for Prometheus are part of prometheus.yaml file and all the alert rules for Alertmanager are configured in prometheus.rules.\nprometheus.yaml: This is the main Prometheus configuration which holds all the scrape configs, service discovery details, storage locations, data retention configs, etc. prometheus.rules: This file contains all the Prometheus alerting rules By externalizing Prometheus configs to a Kubernetes config map, you don’t have to build the Prometheus image whenever you need to add or remove a configuration. You need to update the config map and restart the Prometheus pods to apply the new configuration.\nThe config map with all the Prometheus scrape config and alerting rules gets mounted to the Prometheus container in /etc/prometheus location as prometheus.yaml and prometheus.rules files.\nIn this lab i don\u0026rsquo;t use rometheus.rules. maybe i will set it up in next lab\nOpen the config-map.yaml file and add the yaml below. I wrote this to get the tomcat metrics that I want to get.\n- job_name: \u0026#39;tomcat\u0026#39;\rkubernetes_sd_configs:\r- role: endpoints\rrelabel_configs:\r- source_labels: [__meta_kubernetes_namespace]\raction: replace\rtarget_label: kubernetes_namespace\r- source_labels: [__meta_kubernetes_pod_name]\raction: replace\rtarget_label: kubernetes_pod_name\r- source_labels: [__meta_kubernetes_endpoints_name]\rregex: \u0026#39;tomcat-service\u0026#39;\raction: keep\r- source_labels: [__meta_kubernetes_endpoint_port_name]\rregex: \u0026#39;prometheus\u0026#39;\raction: keep Execute the following command to create the config map in Kubernetes. kubectl create -f config-map.yaml Explain the above configuration First, we need to define the target endpoints. kubectl get endpoints -A as you can see tomcat-service is the target i want to get so i use regex to get only tomcat-service. The next regex I specify to only get the port that tomcat exposes the metric on. Because tomcat will expose 2 ports 8080 and 81828 and only 1 port has the metric, so to avoid confusion I only get the port that contains the metric 8182 named prometheus. Create a Prometheus Deployment. Use default file prometheus-deployment.yaml kubectl create -f prometheus-deployment.yaml Test connecting To Prometheus Dashboard, Exposing Prometheus as a Service NodePort. Create a file named prometheus-service.yaml and copy the following contents. We will expose Prometheus on all kubernetes node IP’s on port 30000. apiVersion: v1\rkind: Service\rmetadata:\rname: prometheus-service\rnamespace: monitoring\rannotations:\rprometheus.io/scrape: \u0026#39;true\u0026#39;\rprometheus.io/port: \u0026#39;9090\u0026#39;\rspec:\rselector: app: prometheus-server\rtype: NodePort ports:\r- port: 8080\rtargetPort: 9090 nodePort: 30000 kubectl apply -f prometheus-service.yaml Now open port 30000, we can see that prometheus has fetched the metrics of the tomcat pods. "
},
{
	"uri": "//localhost:1313/2-setuplab/2.1-createkubernetescluster/",
	"title": "Create Kubernetes Cluster",
	"tags": [],
	"description": "",
	"content": "Prerequisites: Minimum 2GB RAM or more Minimum 2 CPU cores / or 2 vCPU 20 GB free disk space or more Connect to all EC2s and follow the steps below which detail the steps to setup a 2 node cluster.\n1. Prepare EC2 Instances. To optimize costs for this lab, I will use two EC2 instances type t2.medium.\nEach instances have 2 vCPU and 4 GB Memory Check two EC2 are running. Now connect to the instance and follow the steps below.\n2. Set hostname on Each Node Login to to master node and set hostname via hostnamectl command. $ sudo hostnamectl set-hostname K8s_master\r$ exec bash Do the same on the client nodes with name \u0026ldquo;K8s_client\u0026rdquo;.\nAdd the following lines in /etc/hosts file on each node ip k8s-master\rip k8s-client 3. Disable Swap \u0026amp; Add kernel Parameters Execute swapoff and sed command to disable swap. Make sure to run the following commands on all the nodes. $ sudo swapoff -a\r$ sudo sed -i \u0026#39;/ swap / s/^\\(.*\\)$/#\\1/g\u0026#39; /etc/fstab Load the following kernel modules on all the nodes, $ sudo tee /etc/modules-load.d/containerd.conf \u0026lt;\u0026lt;EOF\roverlay\rbr_netfilter\rEOF\r$ sudo modprobe overlay\r$ sudo modprobe br_netfilter Set the following Kernel parameters for Kubernetes, run beneath tee command $ sudo tee /etc/sysctl.d/kubernetes.conf \u0026lt;\u0026lt;EOT\rnet.bridge.bridge-nf-call-ip6tables = 1\rnet.bridge.bridge-nf-call-iptables = 1\rnet.ipv4.ip_forward = 1\rEOT Reload the above changes, run\n$ sudo sysctl --system 4. Install Containerd Runtime In this guide, we are using containerd runtime for our Kubernetes cluster. So, to install containerd, first install its dependencies. $ sudo apt install -y curl gnupg2 software-properties-common apt-transport-https ca-certificates Enable docker repository $ sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmour -o /etc/apt/trusted.gpg.d/docker.gpg\r$ sudo add-apt-repository \u0026#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34; Now, run following apt command to install containerd $ sudo apt update\r$ sudo apt install -y containerd.io Configure containerd so that it starts using systemd as cgroup. $ containerd config default | sudo tee /etc/containerd/config.toml \u0026gt;/dev/null 2\u0026gt;\u0026amp;1\r$ sudo sed -i \u0026#39;s/SystemdCgroup \\= false/SystemdCgroup \\= true/g\u0026#39; /etc/containerd/config.toml Restart and enable containerd service $ sudo systemctl restart containerd\r$ sudo systemctl enable containerd 5. Install Kubectl, Kubeadm and Kubelet Update the apt package index and install packages needed to use the Kubernetes apt repository: sudo apt-get update\r# apt-transport-https may be a dummy package; if so, you can skip that package\rsudo apt-get install -y apt-transport-https ca-certificates curl gpg Download the public signing key for the Kubernetes package repositories. The same signing key is used for all repositories so you can disregard the version in the URL: curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg If the directory /etc/apt/keyrings does not exist, you can create using command: sudo mkdir -p -m 755 /etc/apt/keyrings\nnext #This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list\recho \u0026#39;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /\u0026#39; | sudo tee /etc/apt/sources.list.d/kubernetes.list Update the apt package index, install kubelet, kubeadm and kubectl, and pin their version: sudo apt-get update\rsudo apt-get install -y kubelet kubeadm kubectl\rsudo apt-mark hold kubelet kubeadm kubectl (Optional) Enable the kubelet service before running kubeadm: sudo systemctl enable --now kubelet It will take a lot of time to run the above commands for all 3 nodes. If HA is deployed, the node will be up to 6 or more nodes and it will take 2-3 times more time @@. I am very lazy, so I have written the Script below, just click 1 time to complete. hehe\n#!/bin/bash\rread -p \u0026#34;Enter name: \u0026#34; name\r#-----set hostname\rhostnamectl set-hostname $name\r#exec bash\recho \u0026#34;Enter your cluster: Ex: 10.1.1.1 master\r10.1.1.2 client\rmore\rpress enter + enter if done\u0026#34;\rwhile true; do\rread -p \u0026#34;\u0026gt; \u0026#34; line\rif [ -z \u0026#34;$line\u0026#34; ]; then\rbreak\rfi\recho \u0026#34;$line\u0026#34; | sudo tee -a /etc/hosts \u0026gt; /dev/null\rdone\recho \u0026#34;update /etc/hosts.\u0026#34;\r#------Disable Swap \u0026amp; Add kernel Parameters\rswapoff -a\rsed -i \u0026#39;/ swap / s/^\\(.*\\)$/#\\1/g\u0026#39; /etc/fstab\rtee -a /etc/modules-load.d/containerd.conf \u0026lt;\u0026lt;EOF\roverlay\rbr_netfilter\rEOF\rmodprobe overlay\rmodprobe br_netfilter\rtee -a /etc/sysctl.d/kubernetes.conf \u0026lt;\u0026lt;EOF\rnet.bridge.bridge-nf-call-ip6tables = 1\rnet.bridge.bridge-nf-call-iptables = 1\rnet.ipv4.ip_forward = 1\rEOF\rsysctl --system\r#--Install Containerd Runtime\rapt install -y curl gnupg2 software-properties-common apt-transport-https ca-certificates\rcurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmour -o /etc/apt/trusted.gpg.d/docker.gpg\radd-apt-repository \u0026#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34;\rapt update\rapt install -y containerd.io\rcontainerd config default | sudo tee /etc/containerd/config.toml \u0026gt;/dev/null 2\u0026gt;\u0026amp;1\rsed -i \u0026#39;s/SystemdCgroup \\= false/SystemdCgroup \\= true/g\u0026#39; /etc/containerd/config.toml\rsystemctl restart containerd\rstatus=\u0026#34;$(systemctl show -p SubState containerd | cut -d\u0026#39;=\u0026#39; -f2)\u0026#34;\rif [[ \u0026#34;${status}\u0026#34; == \u0026#34;running\u0026#34; ]]; then\recho \u0026#34;Service containerd is $status\u0026#34;\relse\recho \u0026#34;Service not running - try install containerd again\u0026#34;\rfi\r#-------Install Kubectl, Kubeadm and Kubelet\rapt-get update\rapt-get install -y apt-transport-https ca-certificates curl gpg\rcurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\recho \u0026#39;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /\u0026#39; | sudo tee /etc/apt/sources.list.d/kubernetes.list\rapt-get update\rapt-get install -y kubelet kubeadm kubectl\rapt-mark hold kubelet kubeadm kubectl\rsystemctl enable --now kubelet\rexec bash\recho \u0026#34;-----setup done-----\u0026#34; Initialize Only master node As i mentioned in part 1, i will use new model in which will use the power of new technology that is ebpf. to make it easy to understand i will initialize kubernetes cluster without using kube-proxy. use ebpf instead of iptables. see image below\nRun this command on master node kubeadm init --control-plane-endpoint=k8s-master --upload-certs --skip-phases=addon/kube-proxy cluster initialization successful Start interacting with cluster, run following commands on the master node.\nmkdir -p $HOME/.kube\rsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\rsudo chown $(id -u):$(id -g) $HOME/.kube/config On each worker node, run command kubeadm join\u0026hellip;. 6.Set up Cilium So k8s cluster is almost complete. next we will setup networking for the cluster so they can communicate with ech other. First download and install cilium-cli curl -LO https://github.com/cilium/cilium-cli/releases/download/v0.16.2/cilium-linux-amd64.tar.gz\rcurl -LO https://github.com/cilium/cilium-cli/releases/download/v0.16.2/cilium-linux-amd64.tar.gz.sha256sum See other releases of cilium here\nCheck file integrity\nsha256sum --check cilium-linux-amd64.tar.gz.sha256sum\rcilium-linux-amd64.tar.gz: OK If the result shows \u0026ldquo;FAILED\u0026rdquo;, it means that the integrity of the file is not guaranteed. You should consider using another release.\nInstall cilium Use this command first to check available cilium versions cilium install --list-versions\ntar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin/\rcilium install --version 1.15.0 So we have a fully installed kubernetes cluster and ready to use. "
},
{
	"uri": "//localhost:1313/1-kubernetesarchitecture/1.2-kubernetesnetworking/",
	"title": "Kubernetes Networking",
	"tags": [],
	"description": "",
	"content": "A Pod is the smallest deployable unit in Kubernetes and represents a single instance of an application. Each Pod has its unique IP address and can communicate with other Pods in the same cluster without the need for network address translation (NAT). This means that each Pod can listen on the same port without conflict.Because every component in the cluster is connected to one flat network. In a flat network, all components can communicate with each other without the need for any hardware, such as routers or switches. This is achieved by the Kubernetes network model.\n1. Kubernetes Networking Model The pods are able to interact with each other because of their unique IP addresses. This is the reason why they can communicate with each other over the network.\nHowever, Kubernetes operates across multiple nodes (machines), and pods can be deployed on any of these nodes. This means that pods might be running on different nodes and they need a method to communicate regardless of their location.\nTo facilitate this communication, Kubernetes employs a networking model that ensures pods can talk to each other no matter where they\u0026rsquo;re running. This involves the use of a Container Network Interface (CNI) within Kubernetes, which handles routing traffic between pods, load balancing, and ensuring seamless communication across the cluster.\nOn each node, the Kubernetes network model is implemented using a combination of the container runtime and the CNI plugin. The container runtime sets up the network namespace for each container, while the CNI plugin configures networking rules and policies to enable communication between pods in the cluster.\n2.Container Network Interface (CNI) Kubernetes relies on Container Network Interface (CNI) plugins to assign IP addresses to pods and manage routes between all pods. Kubernetes doesn\u0026rsquo;t ship with a default CNI plugin, however, managed Kubernetes offerings come with a pre-configured CNI. In this lab I will be using cilium. Cilium is based on a technology called eBPF,sounds very interesting 😆. I will explain why I chose it in the next section.\n3.Communication In Kubernetes Network 3.1 Container-to-container communication The containers within the pod (the network namespace) can communicate through localhost and share the same IP address and ports. The \u0026ldquo;network namespaces\u0026rdquo; in Linux allow us to have separate network interfaces and routing tables from the rest of the system. But deploying multiple containers in the same pod is quite rare when deploying applications according to microservices architecture. 3.2 Pod-to-pod communication The nodes in the cluster have their IP address and a CIDR range from where they can assign IP addresses to the pods. A unique CIDR range per node guarantees a unique IP address for every pod in the cluster. The pod-to-pod communication happens through these IP addresses.\nVirtual ethernet devices (veth) connect the two veths across network namespaces. Each pod that runs on the node has a veth pair connecting the pod namespace to the nodes\u0026rsquo; network namespace. In this case, one virtual interface connects to the network namespace in the pod (eth0), and the other virtual interface connects to the root network namespace on the node.\nThe communication between pods in the same node looks like this.\nThe traffic goes from the eth0 interface in the pod to the veth interface on the node side, and then through the virtual bridge to another virtual interface that\u0026rsquo;s connected to eth0 of the destination pod. This is where the Container Network Interface (CNI) comes into play. Amongst other things, the CNI knows how to set up this cross-node communication for pods. And what if the pods are on different nodes. how do they route, Below explains Pod to Pod Communication on different nodes.\nHere is routing table on one of the nodes, we\u0026rsquo;ll see that it specifies where to send the packets. default via 172.18.0.1 dev eth0\r10.244.0.0/24 via 172.18.0.4 dev eth0\r10.244.2.0/24 via 172.18.0.3 dev eth0\r10.244.1.1 dev veth2609878b scope host\r10.244.1.1 dev veth23974a9d scope host\r172.18.0.0/16 dev eth0 proto kernel scope link src 172.18.0.2 The line 10.244.2.0/24 via 172.18.0.3 dev eth0 says that any packets destined to IP\u0026rsquo;s in that CIDR range, which are the pods running on node2, should be sent to 172.18.0.3 via the eth0 interface. 3.3 Pod-to-service communication Pod operations like delete or create can change the ip, it is difficult to assign design if using ip of each pod, we need a durable IP address. Kubernetes solves this problem using a concept of a Kubernetes service.\nWhen created, a service in Kubernetes gives us a virtual IP (cluster IP) backed by one or more pods. When we send traffic to the service IP, it gets routed to one of the backing pod IPs.\nKube-proxy controller connects to the Kubernetes API server and watches for changes to services and pods. As pods get created and destroyed, kube-proxy updates the iptables rules to reflect the changes. Whenever a service/pod gets updated, kube-proxy updates the iptables rules so traffic sent to the service IP gets correctly routed to one of the backing pods.\nThe image above shows services with 3 backend pods, when we send packets from a pod to the service IP, they get filtered through the iptables rules, where the destination IP (service IP) gets changed to one of the backing pod IPs. On the way back (from the destination pod to the source pod), the destination pod IP gets changed back to the service IP, so the source pod thinks it\u0026rsquo;s receiving the response from the service IP. 3.4 Ingress and egress communication Egress communication (traffic existing the cluster) On the way out of the cluster, the iptables ensure the source IP of the pod gets modified to the internal IP address of the node (VM). Typically, when running a cloud-hosted cluster, the nodes have private IPs and run inside a virtual private cloud network (VPC). We need to attach an internet gateway to the VPC network to allow the nodes access to the internet. The gateway performs network address translation (NAT) and changes the internal node IP to the public node IP. NAT allows the response from the internet to be routed back to the node and eventually to the original caller. Ingress communication (traffic entering the cluster) We need a public IP address to get the outside traffic inside the cluster. A Kubernetes LoadBalancer service allows us to get an external IP address. Behind the scenes, a cloud provider specific controller creates an actual load balancer instance in the cloud. The LB has an external IP address to send traffic to or hook up your custom domain. When the traffic arrives at the LB, it gets routed to one of the nodes in the cluster. Then, the iptables rules on the chosen node kick in, do the necessary NAT, and direct the packets to one of the pods that\u0026rsquo;s part of the service. However, we don\u0026rsquo;t want to create a LoadBalancer instance of every service we want to expose. Ideally, we\u0026rsquo;d have a single external IP address and the ability to serve multiple Kubernetes services behind that. "
},
{
	"uri": "//localhost:1313/3-accessibilitytoinstances/3.2-metric-server/",
	"title": "Prometheus - Adapter",
	"tags": [],
	"description": "",
	"content": " Although HPA is installed by default in K8s, to get information for monitoring (CPU, Memory), but if you want to extend by other parameters like jmx then metricserver will not support. So i use prometheus-adapter to scale by metric \u0026ldquo;tomcat_requestcount_total\u0026rdquo; Architecture 1.Download Prometheus Adapter Manifest Use below command to clone yaml file. git clone https://github.com/kubernetes-sigs/prometheus-adapter.git Move to folder deploy/manifests and you\u0026rsquo;ll see all the necessary .yaml file I need to edit some files to fit this lab. First, open the file api-service.yaml apiVersion: apiregistration.k8s.io/v1\rkind: APIService\rmetadata:\rlabels:\rapp.kubernetes.io/component: metrics-adapter\rapp.kubernetes.io/name: prometheus-adapter\rapp.kubernetes.io/version: 0.12.0\r#name: v1beta1.metrics.k8s.io\rname: v1beta1.custom.metrics.k8s.io\rspec:\r#group: metrics.k8s.io\rgroup: custom.metrics.k8s.io\rgroupPriorityMinimum: 100\rinsecureSkipTLSVerify: true\rservice:\rname: prometheus-adapter\rnamespace: monitoring\rversion: v1beta1\rversionPriority: 100 since using custom metrics i changed kubernetes support API to custom.metrics.k8s.io\nNext, replace the configmap.yaml file below to determines which metrics to expose and specifies each of the steps the adapter needs to take to expose a metric in the API.\napiVersion: v1\rdata:\rconfig.yaml: |\rrules:\r#- seriesQuery: \u0026#39;tomcat_requestcount_total\u0026#39;\r- seriesQuery: \u0026#39;{__name__=~\u0026#34;^tomcat_.*\u0026#34;,instance!=\u0026#34;\u0026#34;}\u0026#39;\r#resources: {overrides: {job: {resource: \u0026#34;job\u0026#34;}}}\rresources:\roverrides:\rinstance: {resource: \u0026#34;node\u0026#34;}\rjob: {resource: \u0026#34;job\u0026#34;}\rkubernetes_namespace: {resource: \u0026#34;namespace\u0026#34;}\rkubernetes_pod_name: {resource: \u0026#34;pod\u0026#34;}\rname:\rmatches: \u0026#34;tomcat_requestcount_total\u0026#34;\r#matches: \u0026#34;^tomcat_(.*)$\u0026#34;\rmetricsQuery: \u0026#34;sum(rate(\u0026lt;\u0026lt;.Series\u0026gt;\u0026gt;{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;}[3m])*180) by (\u0026lt;\u0026lt;.GroupBy\u0026gt;\u0026gt;)\u0026#34;\rexternalRules:\r- seriesQuery: \u0026#39;tomcat_requestcount_total\u0026#39;\rresources:\roverrides:\rinstance: {resource: \u0026#34;node\u0026#34;}\rjob: {resource: \u0026#34;job\u0026#34;}\rkubernetes_namespace: {resource: \u0026#34;namespace\u0026#34;}\rkubernetes_pod_name: {resource: \u0026#34;pod\u0026#34;}\rname:\rmatches: \u0026#34;tomcat_requestcount_total\u0026#34;\ras: \u0026#34;tomcat_requestcount_total\u0026#34;\rmetricsQuery: \u0026#34;sum(rate(\u0026lt;\u0026lt;.Series\u0026gt;\u0026gt;{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;}[3m])*180) by (\u0026lt;\u0026lt;.GroupBy\u0026gt;\u0026gt;)\u0026#34;\rkind: ConfigMap\rmetadata:\rlabels:\rapp.kubernetes.io/component: metrics-adapter\rapp.kubernetes.io/name: prometheus-adapter\rapp.kubernetes.io/version: 0.12.0\rname: adapter-config\rnamespace: monitoring To explain a bit, Each rule can be divided into four parts: Discovery which specifies how the adapter should find all Prometheus metrics for this rule.\nAssociation which specifies how the adapter should determine which Kubernetes resources a particular metric is associated with.\nNaming which specifies how the adapter should expose the metric in the custom metrics API.\nQuerying which specifies how a request for a particular metric on one or more Kubernetes objects should be turned into a query to Prometheus.\nRead more on 2. Create Prometheus Adapter Follow command to complete deploy adapter. kubectl create -f prometheus-adapter/deploy/manifests/ As we can see, prometheus-adapter to deploy success and running on cluster ~# kubectl get pod -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES\rapp tomcat-deployment-7fb69d4749-fnpd4 1/1 Running 0 36m 10.0.1.99 k8s-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-system cilium-8phqv 1/1 Running 0 76m 172.22.6.150 k8s-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-system cilium-bgvjv 1/1 Running 0 76m 172.22.11.124 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-system cilium-operator-79bbbc6f56-sdlwq 1/1 Running 0 76m 172.22.6.150 k8s-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-system coredns-6f6b679f8f-6ljtz 1/1 Running 0 78m 10.0.0.8 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-system coredns-6f6b679f8f-btcxx 1/1 Running 0 78m 10.0.0.114 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-system etcd-k8s-master 1/1 Running 0 79m 172.22.11.124 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-system kube-apiserver-k8s-master 1/1 Running 0 79m 172.22.11.124 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-system kube-controller-manager-k8s-master 1/1 Running 30 79m 172.22.11.124 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-system kube-scheduler-k8s-master 1/1 Running 30 79m 172.22.11.124 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rmonitoring prometheus-adapter-777cb6d9d8-5lpp8 1/1 Running 0 32m 10.0.1.58 k8s-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rmonitoring prometheus-deployment-5b9f6c9dd-z8s9f 1/1 Running 0 36m 10.0.1.30 k8s-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Let’s check what all custom metrics are available. ~# kubectl get --raw \u0026#34;/apis/custom.metrics.k8s.io/v1beta1/\u0026#34; | jq .\r{\r\u0026#34;kind\u0026#34;: \u0026#34;APIResourceList\u0026#34;,\r\u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;,\r\u0026#34;groupVersion\u0026#34;: \u0026#34;custom.metrics.k8s.io/v1beta1\u0026#34;,\r\u0026#34;resources\u0026#34;: [\r{\r\u0026#34;name\u0026#34;: \u0026#34;namespaces/tomcat_requestcount_total\u0026#34;,\r\u0026#34;singularName\u0026#34;: \u0026#34;\u0026#34;,\r\u0026#34;namespaced\u0026#34;: false,\r\u0026#34;kind\u0026#34;: \u0026#34;MetricValueList\u0026#34;,\r\u0026#34;verbs\u0026#34;: [\r\u0026#34;get\u0026#34;\r]\r},\r{\r\u0026#34;name\u0026#34;: \u0026#34;pods/tomcat_requestcount_total\u0026#34;,\r\u0026#34;singularName\u0026#34;: \u0026#34;\u0026#34;,\r\u0026#34;namespaced\u0026#34;: true,\r\u0026#34;kind\u0026#34;: \u0026#34;MetricValueList\u0026#34;,\r\u0026#34;verbs\u0026#34;: [\r\u0026#34;get\u0026#34;\r]\r},\r{\r\u0026#34;name\u0026#34;: \u0026#34;nodes/tomcat_requestcount_total\u0026#34;,\r\u0026#34;singularName\u0026#34;: \u0026#34;\u0026#34;,\r\u0026#34;namespaced\u0026#34;: false,\r\u0026#34;kind\u0026#34;: \u0026#34;MetricValueList\u0026#34;,\r\u0026#34;verbs\u0026#34;: [\r\u0026#34;get\u0026#34;\r]\r},\r{\r\u0026#34;name\u0026#34;: \u0026#34;jobs.batch/tomcat_requestcount_total\u0026#34;,\r\u0026#34;singularName\u0026#34;: \u0026#34;\u0026#34;,\r\u0026#34;namespaced\u0026#34;: true,\r\u0026#34;kind\u0026#34;: \u0026#34;MetricValueList\u0026#34;,\r\u0026#34;verbs\u0026#34;: [\r\u0026#34;get\u0026#34;\r]\r}\r]\r} We can see that tomcat_requestcount_total metric is available on all deployment tomcat. Now, let’s check the current value of this metric. ~# kubectl get --raw \u0026#34;/apis/custom.metrics.k8s.io/v1beta1/namespaces/app/pods/*/tomcat_requestcount_total\u0026#34; | jq .\r{\r\u0026#34;kind\u0026#34;: \u0026#34;MetricValueList\u0026#34;,\r\u0026#34;apiVersion\u0026#34;: \u0026#34;custom.metrics.k8s.io/v1beta1\u0026#34;,\r\u0026#34;metadata\u0026#34;: {},\r\u0026#34;items\u0026#34;: [\r{\r\u0026#34;describedObject\u0026#34;: {\r\u0026#34;kind\u0026#34;: \u0026#34;Pod\u0026#34;,\r\u0026#34;namespace\u0026#34;: \u0026#34;app\u0026#34;,\r\u0026#34;name\u0026#34;: \u0026#34;tomcat-deployment-7fb69d4749-fnpd4\u0026#34;,\r\u0026#34;apiVersion\u0026#34;: \u0026#34;/v1\u0026#34;\r},\r\u0026#34;metricName\u0026#34;: \u0026#34;tomcat_requestcount_total\u0026#34;,\r\u0026#34;timestamp\u0026#34;: \u0026#34;2024-09-16T15:53:02Z\u0026#34;,\r\u0026#34;value\u0026#34;: \u0026#34;0\u0026#34;,\r\u0026#34;selector\u0026#34;: null\r}\r]\r} Success, this is what we need to scale in the next part. "
},
{
	"uri": "//localhost:1313/2-setuplab/",
	"title": "Setup Lab",
	"tags": [],
	"description": "",
	"content": "To learn how to create EC2 instances and VPCs with public/private subnets, you can refer to the lab:\nAbout Amazon EC2 Works with Amazon VPC Content Create Kubernetes Cluster Setup Tomcat "
},
{
	"uri": "//localhost:1313/2-setuplab/2.2-setuptomcat/",
	"title": "Setup Tomcat",
	"tags": [],
	"description": "",
	"content": "\rYou must create your own Docker Hub account to use as a place to store images for kubernetes.\nin this lab i will use tomcat metrics to condition scaling in kubernetes and to get that information i will use JMX Exporter What\u0026rsquo;s JMX Exporter, Prometheus JMX Exporter is also a Java agent, which is capable of accessing the MBean server to access the data and transform that data into the Prometheus metric format. Prometheus then scrapes the metrics from the JMX Exporter\u0026rsquo;s default metric storage path, which is /metrics. If you are curious, I have written a tomcat setup file, you can run it on your linux machine to see what information JMX Exporter gets from tomcat. #!/bin/bash\rinstall_jdk() {\rwget https://download.java.net/java/ga/jdk11/openjdk-11_linux-x64_bin.tar.gz\rtar xzvf openjdk-11_linux-x64_bin.tar.gz\rmv jdk-11 /opt/\rupdate-alternatives --install /usr/bin/java java /opt/jdk-11/bin/java 1\rupdate-alternatives --install /usr/bin/javac javac /opt/jdk-11/bin/javac 1\r}\rinstall_tomcat() {\rmkdir -p /opt/tomcat wget -c https://archive.apache.org/dist/tomcat/tomcat-9/v9.0.62/bin/apache-tomcat-9.0.62.tar.gz tar xzvf apache-tomcat-9.0.62.tar.gz -C /opt/tomcat/ --strip-components=1\rgroupadd tomcat\ruseradd --no-create-home --shell /bin/false tomcat -g tomcat\rchown -R tomcat:tomcat /opt/tomcat/\rcat \u0026lt;\u0026lt;EOF \u0026gt; /opt/tomcat/prometheus.yml\r---\rlowercaseOutputLabelNames: true\rlowercaseOutputName: true\rrules:\r- pattern: \u0026#39;Catalina\u0026lt;type=GlobalRequestProcessor, name=\\\u0026#34;(\\w+-\\w+)-(\\d+)\\\u0026#34;\u0026gt;\u0026lt;\u0026gt;(\\w+):\u0026#39;\rname: tomcat_\\$3_total\rlabels:\rport: \u0026#34;\\$2\u0026#34;\rprotocol: \u0026#34;\\$1\u0026#34;\rhelp: Tomcat global \\$3\rtype: COUNTER\r- pattern: \u0026#39;Catalina\u0026lt;j2eeType=Servlet, WebModule=//([-a-zA-Z0-9+\u0026amp;@#/%?=~_|!:.,;]*[-a-zA-Z0-9+\u0026amp;@#/%=~_|]), name=([-a-zA-Z0-9+/$%~_-|!.]*), J2EEApplication=none, J2EEServer=none\u0026gt;\u0026lt;\u0026gt;(requestCount|maxTime|processingTime|errorCount):\u0026#39;\rname: tomcat_servlet_\\$3_total\rlabels:\rmodule: \u0026#34;\\$1\u0026#34;\rservlet: \u0026#34;\\$2\u0026#34;\rhelp: Tomcat servlet \\$3 total\rtype: COUNTER\r- pattern: \u0026#39;Catalina\u0026lt;type=ThreadPool, name=\u0026#34;(\\w+-\\w+)-(\\d+)\u0026#34;\u0026gt;\u0026lt;\u0026gt;(currentThreadCount|currentThreadsBusy|keepAliveCount|pollerThreadCount|connectionCount):\u0026#39;\rname: tomcat_threadpool_\\$3\rlabels:\rport: \u0026#34;\\$2\u0026#34;\rprotocol: \u0026#34;\\$1\u0026#34;\rhelp: Tomcat threadpool \\$3\rtype: GAUGE\r- pattern: \u0026#39;Catalina\u0026lt;type=Manager, host=([-a-zA-Z0-9+\u0026amp;@#/%?=~_|!:.,;]*[-a-zA-Z0-9+\u0026amp;@#/%=~_|]), context=([-a-zA-Z0-9+/$%~_-|!.]*)\u0026gt;\u0026lt;\u0026gt;(processingTime|sessionCounter|rejectedSessions|expiredSessions):\u0026#39;\rname: tomcat_session_\\$3_total\rlabels:\rcontext: \u0026#34;\\$2\u0026#34;\rhost: \u0026#34;\\$1\u0026#34;\rhelp: Tomcat session \\$3 total\rtype: COUNTER\r- pattern: \u0026#39;java.lang\u0026lt;type=OperatingSystem\u0026gt;\u0026lt;\u0026gt;(committed_virtual_memory|free_physical_memory|free_swap_space|total_physical_memory|total_swap_space)_size:\u0026#39;\rname: os_\\$1_bytes\rtype: GAUGE\rattrNameSnakeCase: true\r- pattern: \u0026#39;java.lang\u0026lt;type=OperatingSystem\u0026gt;\u0026lt;\u0026gt;((?!process_cpu_time)\\w+):\u0026#39;\rname: os_\\$1\rtype: GAUGE\rattrNameSnakeCase: true\rEOF\rcat \u0026lt;\u0026lt;EOF \u0026gt; /opt/tomcat/bin/setenv.sh\rCATALINA_OPTS=\u0026#34;\\$CATALINA_OPTS -javaagent:/opt/jmx_prometheus_javaagent-0.19.0.jar=8182:/opt/tomcat/prometheus.yml\u0026#34;\rEnvironment=JAVA_HOME=/opt/jdk-11\rexport JAVA_HOME=/opt/jdk-11\rEOF\rwget https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.19.0/jmx_prometheus_javaagent-0.19.0.jar\rmv jmx_prometheus_javaagent-0.19.0.jar /opt/\r}\rinstall_jdk\rinstall_tomcat Run this command to start tomcat server. /opt/tomcat/bin/startup.sh Tomcat run on port 8080 and metrics expose on port 8182.\nMetrics will look like this: Create Tomcat Images. Your machine has Docker installed. Next, execute the command below to create the directory and Dockerfile. mkdir tomcat\rcd tomcat\r#Prepare the necessary files. Versions may vary depending on your purpose.\rwget https://download.java.net/java/ga/jdk11/openjdk-11_linux-x64_bin.tar.gz\rwget -c https://archive.apache.org/dist/tomcat/tomcat-9/v9.0.62/bin/apache-tomcat-9.0.62.tar.gz\rwget https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.19.0/jmx_prometheus_javaagent-0.19.0.jar\rtouch Dockerfile Add the lines below to Dockerfile FROM ubuntu:20.04\rRUN apt-get update \u0026amp;\u0026amp; apt-get install -y tar \\\r\u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/*\rCOPY openjdk-11_linux-x64_bin.tar.gz /tmp/\rRUN mkdir -p /opt/jdk-11 \\\r\u0026amp;\u0026amp; tar xzvf /tmp/openjdk-11_linux-x64_bin.tar.gz -C /opt/jdk-11/ --strip-components=1 \\\r\u0026amp;\u0026amp; update-alternatives --install /usr/bin/java java /opt/jdk-11/bin/java 1 \\\r\u0026amp;\u0026amp; update-alternatives --install /usr/bin/javac javac /opt/jdk-11/bin/javac 1 COPY apache-tomcat-9.0.62.tar.gz /tmp/ RUN mkdir -p /opt/tomcat \\\r\u0026amp;\u0026amp; tar xzvf /tmp/apache-tomcat-9.0.62.tar.gz -C /opt/tomcat/ --strip-components=1 RUN groupadd tomcat \\\r\u0026amp;\u0026amp; useradd --no-create-home --shell /bin/false tomcat -g tomcat \\\r\u0026amp;\u0026amp; chown -R tomcat:tomcat /opt/tomcat/\rCOPY prometheus.yml /opt/tomcat/\rCOPY setenv.sh /opt/tomcat/bin/\rCOPY jmx_prometheus_javaagent-0.19.0.jar /opt/\rEXPOSE 8182 8080\rCMD [\u0026#34;/opt/tomcat/bin/catalina.sh\u0026#34;, \u0026#34;run\u0026#34;] Create Docker image using the following command: docker build -t my-tomcat-image . Image created successfully. In your machine, login on your Docker Hub. You will be asked to enter a username and password. The password will be encrypted and stored locally.\ndocker login Tag images with names in Docker Hub. #docker tag \u0026lt;image_id\u0026gt; \u0026lt;dockerhub_username\u0026gt;/\u0026lt;repository_name\u0026gt;:\u0026lt;tag\u0026gt;\rdocker tag 282612a56d35 nessa13044/tomcat-k8s:v1 Push Image to Docker Hub. docker push nessa13044/tomcat-k8s:v1 check images are ready on Docker Hub. "
},
{
	"uri": "//localhost:1313/1-kubernetesarchitecture/1.3-cilium/",
	"title": "Cilium",
	"tags": [],
	"description": "",
	"content": "Overview There are dozens of CNIs available for Kubernetes but, their features, scale, and performance vary greatly. Many of them rely on a legacy technology (iptables) that cannot handle the scale and churn of Kubernetes environments leading to increased latency and reduced throughput.\nCilium is an open source project that provides a networking and security solution based on eBPF (extended Berkeley Packet Filter) technology for containers and Kubernetes environments.\nFeature that impressed me the most was the Networking, Security and Observability Below is Hubble, a network monitoring and observation tool, part of the Cilium project.\nMore on CILium Network Performance The above image shows how traditional packet filtering works using iptables and using the new technology ebpf in kubernetes. Read more here "
},
{
	"uri": "//localhost:1313/3-accessibilitytoinstances/3.3-hpa/",
	"title": "Horizontal Pod Autoscaling - HPA",
	"tags": [],
	"description": "",
	"content": " Custom metrics should be labeled and exposed in a way that HPA can query. To use custom metrics, create or modify an HPA resource. Below is an example YAML configuration for HPA that scales based on a custom metric: apiVersion: autoscaling/v2\rkind: HorizontalPodAutoscaler\rmetadata:\rname: hpa-tomcat\rspec:\rscaleTargetRef:\rapiVersion: apps/v1\rkind: Deployment\rname: tomcat-deployment\rminReplicas: 1\rmaxReplicas: 3\rmetrics:\r- type: Pods\rpods:\rmetric:\rname: \u0026#34;tomcat_requestcount_total\u0026#34;\rtarget:\rtype: AverageValue\raverageValue: 10000m Create HPA with the following command. kubectl create -f hpa-tomcat.yaml -n app Note: You must deploy the hpa in the same namespaces as the pods you want to scale.\nNow, let\u0026rsquo;s check the current status of HPA as follows. ~# kubectl get hpa -A NAMESPACE NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS app hpa-tomcat Deployment/tomcat-deployment 0/10 1 3 1 ~# kubectl describe hpa -A\rName: hpa-tomcat\rNamespace: app\rLabels: \u0026lt;none\u0026gt;\rAnnotations: \u0026lt;none\u0026gt;\rCreationTimestamp: Mon, 16 Sep 2024 16:42:58 +0000\rReference: Deployment/tomcat-deployment\rMetrics: ( current / target )\r\u0026#34;tomcat_requestcount_total\u0026#34; on pods: 14400m / 10\rMin replicas: 1\rMax replicas: 3\rDeployment pods: 1 current / 2 desired\rConditions:\rType Status Reason Message\r---- ------ ------ -------\rAbleToScale True SucceededRescale the HPA controller was able to update the target scale to 2\rScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from pods metric tomcat_requestcount_total\rScalingLimited False DesiredWithinRange the desired count is within the acceptable range\rEvents:\rType Reason Age From Message\r---- ------ ---- ---- -------\rNormal SuccessfulRescale 5s (x2 over 4d23h) horizontal-pod-autoscaler New size: 2; reason: pods metric tomcat_requestcount_total above target "
},
{
	"uri": "//localhost:1313/3-accessibilitytoinstances/",
	"title": "Monitoring",
	"tags": [],
	"description": "",
	"content": " In the previous part i created my own tomcat image and in this session i will use it to deploy on kubernetes. next, use the tools to monitor and get the tomcat_requestcount_total parameter that i need to scale the application. Content Monitoring with Prometheus Prometheus Adapter Horizontal Pod Autoscaling - HPA "
},
{
	"uri": "//localhost:1313/4-cleanup/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": "We will take the following steps to delete the resources we created in this exercise.\nDelete EC2 instance Go to EC2 service management console Click Instances. Select both Public Linux Instance and Private Windows Instance instances. Click Instance state. Click Terminate instance, then click Terminate to confirm. "
},
{
	"uri": "//localhost:1313/3-accessibilitytoinstances/3.4-demo/",
	"title": "Demo",
	"tags": [],
	"description": "",
	"content": " Use this command to create a pod named \u0026ldquo;loadgenerator\u0026rdquo; and continuously send HTTP requests to an IP address (here tomcat\u0026rsquo;s service 10.98.29.31:8080) to simulate increased load on the service at that address. kubectl run -it --rm --restart=Never loadgenerator --image=busybox -- sh -c \u0026#34;while true; do wget -O - -q http://10.98.29.31:8080; done\u0026#34; Note: Replace with your tomcat services.\nOpen another terminal to see the whole HPA scaling process. kubectl get hpa -A -w As you can see, the larger the number of requests (more than 30k here), the number of pods automatically expanded to 3 copies as we configured. "
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]